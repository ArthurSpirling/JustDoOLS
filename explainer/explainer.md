# Why Simple Models Almost Always Work Best: A Non-Technical Explainer

Our paper has some technical elements, though the underlying ideas are straightforward. Here we give a non-technical account of our claims and our evidence for those claims.  The intended audience is one of social scientists who are potentially unfamiliar with recent developments in machine learning---especially "deep learning". Below each section you will find a "too long, didn't read" summary. 

## 1. Machine Learning and Prediction

Suppose you wanted to predict conflict between nations, or the winner of a general election, or the outcomes for children in the state social welfare system.  How would you do it?  First, let's be clear about what we mean by *predict*.  What we care about is good *forecasts*: you give me a bunch of conditions---values of variables (the *X*s)---and I tell you what will happen to the outcome (*Y*).  For instance, you tell me the state of the economy (*X1*), how popular the candidates are (*X2*), whether there are incumbents running (*X3*) and so on...and I tell you who will win (*Y*). Importantly, we are not trying to *explain* how candidates win votes with some theoretical model of the world which we then take to data.  We are just trying to do the best job we can---the most *accurate* job we can---of saying what will happen. So, for example, we don't think that one poll has a *causal* effect on another poll a month later.  But that earlier poll is probably *informative* about what will happen in that later poll and then in the election itself.

What methods would we used to *produce* the prediction?  The answer over the past 50 years or so has been *machine learning*.  Exactly what we mean by "machine learning" depends on the field and the problem, but the basic idea is to *automate* some of the model-building and model-fitting we would otherwise do by hand.  To be more specific, we are interested here in *supervised learning*. This is the situation where we ``train" a model based on pre-existing examples of a phenomenon---their inputs (the *X*s) and their outcomes (the *Y*s).  The "machine"---which we say more about below---will hopefully learn how the inputs are related to the outcomes.  The key, and the reason we use supervised learning, is because it can try all sorts of possible relationships between *X* and *Y* that we might never have thought of or imagined.  

That is, rather than us saying "try a model where the incumbent wins if the unemployment rate is low enough, and the GDP growth is high enough, and the US is not involved in any foreign wars", our supervised approach will try *that* model, and millions of other such models that all vary from each other.  That variation comes from different functions it is using to combine the variables, including multiplying them by each other, squaring them, cubing them and so on. And it will do all this *automatically* and in a way that optimizes the correctness of its predictions. We can evaluate how well the machine has done by checking its accuracy on a set of data we kept back from the set of observations we let it use to learn the relationship between *X* and *Y*. For that ``held out" data, we give it the value of the inputs and ask for its best guess of the outcomes (*Y*)---which we know, but that do not show it.  If it does well enough on that *test set* of data, we can deploy it in the real world.  

So, for example, we might train an election predicting model on US presidential elections from the following post-war years: 1948, 1956, 1964, ... ,2012, 2020. That is, every second election.  The machine will consider all the variables we have at hand---the prevailing unemployment rate, the inflation rate of the election year, the Fed interest rate etc. It will use them to predict what happened in those elections---say, ``did the Democrat win?".  Then we see how well that model performs on our *test* set: the *X*s and *Y*s for 1952, 1960, 1968, ..., 2008, 2016.  If we think it looks accurate enough, we plug in the *X*s for 2024, and see what it predicts.

**tl;dr: machine learning is about prediction. Machine learning models can automatically try all sorts of relationships between inputs and outcomes in an effort to forecast well---these include models we would never have thought of**

## 2. Supervised learning in Political Science

How is this working out?  Thinking broadly, machine learning has revolutionized science. We see it everywhere: from image analysis to geology to medicine to physics.  A special class of complicated models called (neural) nets has been particularly successful, and a related field of "[deep learning](https://www.nature.com/articles/nature14539)" has grown up around them. Many of the breakthroughs we see today in "Artificial Intelligence" and "Large Language Models" are based on those techniques.

What about in social science?  Here the news is less positive.  

