# Model Complexity for Supervised Learning: Why Simple Models Almost Always Work Best, And Why It Matters for Applied Research

Paper and related materials for [Morucci](https://marcomorucci.com/bio/) and [Spirling](https://arthurspirling.org/) (2024).  The abstract for the paper is as follows: 

> Inspired by  other fields, political scientists have embraced the use of supervised learning for prediction, inference, measurement and description. In doing so, they typically use flexible models of considerable complexity that have proved successful in non-social science settings. Yet there appear to be profound limits to the payoff of such approaches, at least relative to the alternative of using very simple (generalized linear) models for such tasks. We explain why this is, how to identify the problems for which this will be true, and what to do about it.  We show that the *intrinsic dimension* of political science data is low, and this means returns to complexity are muted or non-existant.  We provide a theory of ``data curation" to explain this state of affairs.  Our approach allows us to diagnose when simple models are optimal, and to provide advice for practitioners seeking to use machine learning


You can find the working paper [here](https://arthurspirling.org/documents/MorucciSpirling_JustDoOLS.pdf) and a non-technical explainer [here](https://github.com/ArthurSpirling/JustDoOLS/blob/main/explainer/explainer.md).

Shortly, we will release [`PyTorch`](https://pytorch.org/) code that can be used for the purposes we describe in the paper.  Watch this space!

Comments welcome: please send us an email, or open an issue above. 
